{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7c098ce",
   "metadata": {},
   "source": [
    "# Example Custom TF Model for RayTradingEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae32931",
   "metadata": {},
   "source": [
    "- Rllib supports customized TensorFlow Keras and also PyTorch Models\n",
    "- Example uses a sample custom model from rllib tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebacc03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import pprint\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray_trading_env import RayTradingEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0107acfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 18:55:54,082\tWARNING services.py:2002 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.8.8', ray_version='1.13.0', ray_commit='e4ce38d001dbbe09cd21c497fedd03d692b2be3e', address_info={'node_ip_address': '10.1.150.226', 'raylet_ip_address': '10.1.150.226', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-06-27_18-55-53_599559_56084/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-06-27_18-55-53_599559_56084/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2022-06-27_18-55-53_599559_56084', 'metrics_export_port': 55784, 'gcs_address': '10.1.150.226:47068', 'address': '10.1.150.226:47068', 'node_id': '64285f3596a3491064a381176173629dd6e63b8f53441762dc456aef'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4b59d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config dict for Rllib (default env parameters)\n",
    "config = {\n",
    "    \"env\": RayTradingEnvironment,\n",
    "    \"create_env_on_driver\": True,\n",
    "    # horizon needs to be specified if the env has no \n",
    "    # max_number_of_steps-like parameter\n",
    "    \"horizon\" : 252,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17c1d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping criteria for tune\n",
    "stop = {\n",
    "    \"training_iteration\": 10,\n",
    "    \"episode_reward_mean\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47852439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02d183c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this is an example model from Rllib tutorials\n",
    "# https://github.com/sven1977/rllib_tutorials/blob/main/ray_summit_2021/tutorial_notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c590f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "\n",
    "tf1, tf, tf_version = try_import_tf()\n",
    "\n",
    "class MyKerasModel(TFModelV2):\n",
    "    \"\"\"Custom model for policy gradient algorithms.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        \"\"\"Build a simple [16, 16]-MLP (+ value branch).\"\"\"\n",
    "        super(MyKerasModel, self).__init__(obs_space, action_space,\n",
    "                                           num_outputs, model_config, name)\n",
    "        \n",
    "        # Keras Input layer.\n",
    "        self.inputs = tf.keras.layers.Input(\n",
    "            shape=obs_space.shape, name=\"observations\")\n",
    "\n",
    "        # Hidden layer (shared by action logits outputs and value output).\n",
    "        layer_1 = tf.keras.layers.Dense(\n",
    "            16,\n",
    "            name=\"layer1\",\n",
    "            activation=tf.nn.relu)(self.inputs)\n",
    "        \n",
    "        # Action logits output.\n",
    "        logits = tf.keras.layers.Dense(\n",
    "            num_outputs,\n",
    "            name=\"out\",\n",
    "            activation=None)(layer_1)\n",
    "\n",
    "        # \"Value\"-branch (single node output).\n",
    "        # Used by several RLlib algorithms (e.g. PPO) to calculate an observation's value.\n",
    "        value_out = tf.keras.layers.Dense(\n",
    "            1,\n",
    "            name=\"value\",\n",
    "            activation=None)(layer_1)\n",
    "\n",
    "        # The actual Keras model:\n",
    "        self.base_model = tf.keras.Model(self.inputs,\n",
    "                                         [logits, value_out])\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \"\"\"Custom-define your forard pass logic here.\"\"\"\n",
    "        # Pass inputs through our 2 layers and calculate the \"value\"\n",
    "        # of the observation and store it for when `value_function` is called.\n",
    "        logits, self.cur_value = self.base_model(input_dict[\"obs\"])\n",
    "        return logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        \"\"\"Implement the value branch forward pass logic here:\n",
    "        \n",
    "        We will just return the already calculated `self.cur_value`.\n",
    "        \"\"\"\n",
    "        assert self.cur_value is not None, \"Must call `forward()` first!\"\n",
    "        return tf.reshape(self.cur_value, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea3cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update confic dist with \"custom model\": <MyModel>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0b4c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our custom model and re-run the experiment.\n",
    "config.update({\n",
    "    \"model\": {\n",
    "        \"custom_model\": MyKerasModel,\n",
    "        \"custom_model_config\": {\n",
    "            #\"layers\": [128, 128],\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2866332b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'env': ray_trading_env.RayTradingEnvironment,\n",
       " 'create_env_on_driver': True,\n",
       " 'horizon': 252,\n",
       " 'model': {'custom_model': __main__.MyKerasModel, 'custom_model_config': {}}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "140bbbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Tune with custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f999833f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=57367)\u001b[0m INFO:ray_trading_env:ray_trading_env logger started.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=57367)\u001b[0m 2022-06-27 19:07:13,165\tINFO trainer.py:2332 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=57367)\u001b[0m 2022-06-27 19:07:13,166\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=57367)\u001b[0m 2022-06-27 19:07:13,166\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=57367)\u001b[0m INFO:ray_trading_env:got data for AAPL...\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=57367)\u001b[0m /opt/conda/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=57367)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=57367)\u001b[0m 2022-06-27 19:07:13,357\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=57367)\u001b[0m 2022-06-27 19:07:13,737\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-06-27 19:07:13 (running for 00:00:03.26)<br>Memory usage on this node: 47.6/251.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/24 CPUs, 0/2 GPUs, 0.0/172.06 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:A100)<br>Result logdir: /home/jovyan/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                           </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_RayTradingEnvironment_58882_00000</td><td>RUNNING </td><td>10.1.150.226:57367</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=57400)\u001b[0m INFO:ray_trading_env:ray_trading_env logger started.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=57401)\u001b[0m INFO:ray_trading_env:ray_trading_env logger started.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=57400)\u001b[0m INFO:ray_trading_env:got data for AAPL...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=57400)\u001b[0m /opt/conda/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=57400)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=57400)\u001b[0m 2022-06-27 19:07:15,930\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=57401)\u001b[0m INFO:ray_trading_env:got data for AAPL...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=57401)\u001b[0m /opt/conda/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=57401)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=57401)\u001b[0m 2022-06-27 19:07:16,014\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=57367)\u001b[0m 2022-06-27 19:07:17,682\tWARNING deprecation.py:46 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_RayTradingEnvironment_58882_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-06-27_19-07-18\n",
      "  done: false\n",
      "  episode_len_mean: 252.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.5642506575628538\n",
      "  episode_reward_mean: -0.16439253446926522\n",
      "  episode_reward_min: -0.6806104443722844\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 14\n",
      "  experiment_id: 0446e3ff69ca4bf89b20ba27250a3f95\n",
      "  hostname: jupyter-fewald\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9452173113822937\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0008030488970689476\n",
      "          model: {}\n",
      "          policy_loss: -0.0009901912417262793\n",
      "          total_loss: 0.4790833294391632\n",
      "          vf_explained_var: -0.9474313855171204\n",
      "          vf_loss: 0.4799129068851471\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.1.150.226\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.142857142857146\n",
      "    ram_util_percent: 19.04285714285714\n",
      "  pid: 57367\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04025979497205133\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14930078829603752\n",
      "    mean_inference_ms: 0.3474214445168468\n",
      "    mean_raw_obs_processing_ms: 0.06318759584593688\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 252.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.5642506575628538\n",
      "    episode_reward_mean: -0.16439253446926522\n",
      "    episode_reward_min: -0.6806104443722844\n",
      "    episodes_this_iter: 14\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      episode_reward:\n",
      "      - -0.3379721612959513\n",
      "      - -0.6806104443722844\n",
      "      - 0.003088205368456925\n",
      "      - -0.02114996488412048\n",
      "      - -0.06657939552775471\n",
      "      - 0.05542199867438782\n",
      "      - 0.5329976237006139\n",
      "      - -0.4672494376568355\n",
      "      - 0.5642506575628538\n",
      "      - -0.15980689135034654\n",
      "      - -0.08961138817278139\n",
      "      - -0.46503432474666795\n",
      "      - -0.6647316936536749\n",
      "      - -0.5045082662156086\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.04025979497205133\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.14930078829603752\n",
      "      mean_inference_ms: 0.3474214445168468\n",
      "      mean_raw_obs_processing_ms: 0.06318759584593688\n",
      "  time_since_restore: 4.816148042678833\n",
      "  time_this_iter_s: 4.816148042678833\n",
      "  time_total_s: 4.816148042678833\n",
      "  timers:\n",
      "    learn_throughput: 4562.537\n",
      "    learn_time_ms: 876.705\n",
      "    load_throughput: 26255424.1\n",
      "    load_time_ms: 0.152\n",
      "    training_iteration_time_ms: 4814.163\n",
      "    update_time_ms: 1.178\n",
      "  timestamp: 1656356838\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: '58882_00000'\n",
      "  warmup_time: 0.5864734649658203\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-06-27 19:07:18 (running for 00:00:08.12)<br>Memory usage on this node: 48.1/251.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/24 CPUs, 0/2 GPUs, 0.0/172.06 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:A100)<br>Result logdir: /home/jovyan/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_RayTradingEnvironment_58882_00000</td><td>RUNNING </td><td>10.1.150.226:57367</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.81615</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-0.164393</td><td style=\"text-align: right;\">            0.564251</td><td style=\"text-align: right;\">            -0.68061</td><td style=\"text-align: right;\">               252</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_RayTradingEnvironment_58882_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-06-27_19-07-25\n",
      "  done: false\n",
      "  episode_len_mean: 252.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6297300329195069\n",
      "  episode_reward_mean: -0.2990044221292842\n",
      "  episode_reward_min: -1.8460572648061357\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 62\n",
      "  experiment_id: 0446e3ff69ca4bf89b20ba27250a3f95\n",
      "  hostname: jupyter-fewald\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02500000037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9188064336776733\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0007729936623945832\n",
      "          model: {}\n",
      "          policy_loss: -0.0070674181915819645\n",
      "          total_loss: 0.13114511966705322\n",
      "          vf_explained_var: -0.8726165890693665\n",
      "          vf_loss: 0.13819320499897003\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 10.1.150.226\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 16000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.75\n",
      "    ram_util_percent: 19.1\n",
      "  pid: 57367\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04099529887017963\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1521623976828481\n",
      "    mean_inference_ms: 0.35579118568466056\n",
      "    mean_raw_obs_processing_ms: 0.06509640817784429\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 252.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.6297300329195069\n",
      "    episode_reward_mean: -0.2990044221292842\n",
      "    episode_reward_min: -1.8460572648061357\n",
      "    episodes_this_iter: 16\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      episode_reward:\n",
      "      - -0.3379721612959513\n",
      "      - -0.6806104443722844\n",
      "      - 0.003088205368456925\n",
      "      - -0.02114996488412048\n",
      "      - -0.06657939552775471\n",
      "      - 0.05542199867438782\n",
      "      - 0.5329976237006139\n",
      "      - -0.4672494376568355\n",
      "      - 0.5642506575628538\n",
      "      - -0.15980689135034654\n",
      "      - -0.08961138817278139\n",
      "      - -0.46503432474666795\n",
      "      - -0.6647316936536749\n",
      "      - -0.5045082662156086\n",
      "      - -0.6959916866578564\n",
      "      - -0.20023182030331133\n",
      "      - -0.1254227962856167\n",
      "      - -0.6679815337819521\n",
      "      - 0.6297300329195069\n",
      "      - 0.11974415181767031\n",
      "      - -0.3201536696979149\n",
      "      - -0.2556826871229522\n",
      "      - -1.047526466973906\n",
      "      - -0.42443128975275757\n",
      "      - -0.2752785842543405\n",
      "      - -0.42443204801099527\n",
      "      - -0.6036961848005238\n",
      "      - -0.1904800717792428\n",
      "      - -0.05599052450788253\n",
      "      - 0.23056194241941058\n",
      "      - 0.1523665197470388\n",
      "      - -0.43240363945260835\n",
      "      - -1.8460572648061357\n",
      "      - -0.32638067907626805\n",
      "      - -0.5992349969816922\n",
      "      - -0.6543036214341487\n",
      "      - -0.35190561403123155\n",
      "      - 0.04912007577457282\n",
      "      - -0.21302331040259148\n",
      "      - -0.49782563493488907\n",
      "      - -0.6713358041443319\n",
      "      - -0.32474840482787065\n",
      "      - -0.7829030780783711\n",
      "      - 0.2787089226412435\n",
      "      - -0.22012212646332407\n",
      "      - -0.4663014360205356\n",
      "      - -1.1580545312989448\n",
      "      - -0.8785140621030152\n",
      "      - -0.43501902429094813\n",
      "      - -0.6294790701989924\n",
      "      - -0.03828869722774486\n",
      "      - -0.41520279869668025\n",
      "      - -0.30937119980825944\n",
      "      - -0.3257735415571911\n",
      "      - 0.16963266419089534\n",
      "      - 0.5315599634490733\n",
      "      - -1.111008781216646\n",
      "      - 0.0688933604291986\n",
      "      - 0.28380827897561334\n",
      "      - -0.48264536351735277\n",
      "      - 0.055252238233984924\n",
      "      - -0.3489547955450895\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.04099529887017963\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1521623976828481\n",
      "      mean_inference_ms: 0.35579118568466056\n",
      "      mean_raw_obs_processing_ms: 0.06509640817784429\n",
      "  time_since_restore: 11.167069673538208\n",
      "  time_this_iter_s: 2.1636626720428467\n",
      "  time_total_s: 11.167069673538208\n",
      "  timers:\n",
      "    learn_throughput: 5009.695\n",
      "    learn_time_ms: 798.452\n",
      "    load_throughput: 23464637.762\n",
      "    load_time_ms: 0.17\n",
      "    training_iteration_time_ms: 2788.672\n",
      "    update_time_ms: 1.457\n",
      "  timestamp: 1656356845\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: '58882_00000'\n",
      "  warmup_time: 0.5864734649658203\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-06-27 19:07:25 (running for 00:00:14.58)<br>Memory usage on this node: 48.1/251.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/24 CPUs, 0/2 GPUs, 0.0/172.06 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:A100)<br>Result logdir: /home/jovyan/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_RayTradingEnvironment_58882_00000</td><td>RUNNING </td><td>10.1.150.226:57367</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         11.1671</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-0.299004</td><td style=\"text-align: right;\">             0.62973</td><td style=\"text-align: right;\">            -1.84606</td><td style=\"text-align: right;\">               252</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_RayTradingEnvironment_58882_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-06-27_19-07-27\n",
      "  done: true\n",
      "  episode_len_mean: 252.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.6297300329195069\n",
      "  episode_reward_mean: -0.31896347714922946\n",
      "  episode_reward_min: -1.8460572648061357\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 78\n",
      "  experiment_id: 0446e3ff69ca4bf89b20ba27250a3f95\n",
      "  hostname: jupyter-fewald\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012500000186264515\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9446378350257874\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.000565099238883704\n",
      "          model: {}\n",
      "          policy_loss: 0.00014079088578000665\n",
      "          total_loss: 0.08317603915929794\n",
      "          vf_explained_var: -0.9823827743530273\n",
      "          vf_loss: 0.08302818983793259\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 10.1.150.226\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 20000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.666666666666664\n",
      "    ram_util_percent: 19.1\n",
      "  pid: 57367\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04100841959306089\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15219998948740532\n",
      "    mean_inference_ms: 0.3560332566950813\n",
      "    mean_raw_obs_processing_ms: 0.06515876595209595\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 252.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.6297300329195069\n",
      "    episode_reward_mean: -0.31896347714922946\n",
      "    episode_reward_min: -1.8460572648061357\n",
      "    episodes_this_iter: 16\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      episode_reward:\n",
      "      - -0.3379721612959513\n",
      "      - -0.6806104443722844\n",
      "      - 0.003088205368456925\n",
      "      - -0.02114996488412048\n",
      "      - -0.06657939552775471\n",
      "      - 0.05542199867438782\n",
      "      - 0.5329976237006139\n",
      "      - -0.4672494376568355\n",
      "      - 0.5642506575628538\n",
      "      - -0.15980689135034654\n",
      "      - -0.08961138817278139\n",
      "      - -0.46503432474666795\n",
      "      - -0.6647316936536749\n",
      "      - -0.5045082662156086\n",
      "      - -0.6959916866578564\n",
      "      - -0.20023182030331133\n",
      "      - -0.1254227962856167\n",
      "      - -0.6679815337819521\n",
      "      - 0.6297300329195069\n",
      "      - 0.11974415181767031\n",
      "      - -0.3201536696979149\n",
      "      - -0.2556826871229522\n",
      "      - -1.047526466973906\n",
      "      - -0.42443128975275757\n",
      "      - -0.2752785842543405\n",
      "      - -0.42443204801099527\n",
      "      - -0.6036961848005238\n",
      "      - -0.1904800717792428\n",
      "      - -0.05599052450788253\n",
      "      - 0.23056194241941058\n",
      "      - 0.1523665197470388\n",
      "      - -0.43240363945260835\n",
      "      - -1.8460572648061357\n",
      "      - -0.32638067907626805\n",
      "      - -0.5992349969816922\n",
      "      - -0.6543036214341487\n",
      "      - -0.35190561403123155\n",
      "      - 0.04912007577457282\n",
      "      - -0.21302331040259148\n",
      "      - -0.49782563493488907\n",
      "      - -0.6713358041443319\n",
      "      - -0.32474840482787065\n",
      "      - -0.7829030780783711\n",
      "      - 0.2787089226412435\n",
      "      - -0.22012212646332407\n",
      "      - -0.4663014360205356\n",
      "      - -1.1580545312989448\n",
      "      - -0.8785140621030152\n",
      "      - -0.43501902429094813\n",
      "      - -0.6294790701989924\n",
      "      - -0.03828869722774486\n",
      "      - -0.41520279869668025\n",
      "      - -0.30937119980825944\n",
      "      - -0.3257735415571911\n",
      "      - 0.16963266419089534\n",
      "      - 0.5315599634490733\n",
      "      - -1.111008781216646\n",
      "      - 0.0688933604291986\n",
      "      - 0.28380827897561334\n",
      "      - -0.48264536351735277\n",
      "      - 0.055252238233984924\n",
      "      - -0.3489547955450895\n",
      "      - -0.33483442529954704\n",
      "      - -0.29349066947623076\n",
      "      - -0.014287170814982375\n",
      "      - -0.8387660663896183\n",
      "      - -0.35177312108246617\n",
      "      - -0.4455056175848912\n",
      "      - -0.5356710834167927\n",
      "      - -0.45694106860587647\n",
      "      - -0.20647131383287004\n",
      "      - -0.21142640034777926\n",
      "      - -0.6241884260647544\n",
      "      - -0.42849369567614365\n",
      "      - -0.5835177263390455\n",
      "      - -0.8024373144249969\n",
      "      - -0.1343336884934186\n",
      "      - -0.07873925777486365\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.04100841959306089\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.15219998948740532\n",
      "      mean_inference_ms: 0.3560332566950813\n",
      "      mean_raw_obs_processing_ms: 0.06515876595209595\n",
      "  time_since_restore: 13.21593952178955\n",
      "  time_this_iter_s: 2.0488698482513428\n",
      "  time_total_s: 13.21593952178955\n",
      "  timers:\n",
      "    learn_throughput: 5076.889\n",
      "    learn_time_ms: 787.884\n",
      "    load_throughput: 23790720.363\n",
      "    load_time_ms: 0.168\n",
      "    training_iteration_time_ms: 2640.048\n",
      "    update_time_ms: 1.377\n",
      "  timestamp: 1656356847\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: '58882_00000'\n",
      "  warmup_time: 0.5864734649658203\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-06-27 19:07:27 (running for 00:00:16.71)<br>Memory usage on this node: 48.1/251.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/24 CPUs, 0/2 GPUs, 0.0/172.06 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:A100)<br>Result logdir: /home/jovyan/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                           </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_RayTradingEnvironment_58882_00000</td><td>TERMINATED</td><td>10.1.150.226:57367</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         13.2159</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-0.318963</td><td style=\"text-align: right;\">             0.62973</td><td style=\"text-align: right;\">            -1.84606</td><td style=\"text-align: right;\">               252</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 19:07:27,877\tINFO tune.py:747 -- Total run time: 17.39 seconds (16.67 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fdb382c4a00>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune.run(\n",
    "    \"PPO\",\n",
    "    config=config,  # for torch users: config=dict(config, **{\"framework\": \"torch\"}),\n",
    "    stop={\n",
    "        \"training_iteration\": 5,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6105c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
