{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c93ecc",
   "metadata": {},
   "source": [
    "# Example Custom TF Model for RayTradingEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dffdd5",
   "metadata": {},
   "source": [
    "- Rllib supports customized TensorFlow Keras and also PyTorch Models\n",
    "- Example uses a sample custom model from rllib tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc99eb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ray_trading_env:ray_trading_env logger started.\n"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import pprint\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray_trading_env import RayTradingEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4475f89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 19:08:19,869\tWARNING services.py:2002 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67096576 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.8.8', ray_version='1.13.0', ray_commit='e4ce38d001dbbe09cd21c497fedd03d692b2be3e', address_info={'node_ip_address': '10.1.150.226', 'raylet_ip_address': '10.1.150.226', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-06-27_19-08-18_191563_57503/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-06-27_19-08-18_191563_57503/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2022-06-27_19-08-18_191563_57503', 'metrics_export_port': 38393, 'gcs_address': '10.1.150.226:47157', 'address': '10.1.150.226:47157', 'node_id': '012b348b79661ce6d8968c6b5f89375d38d8a36b7fcdfeb506fd2e9a'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7449d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config dict for Rllib (default env parameters)\n",
    "config = {\n",
    "    \"env\": RayTradingEnvironment,\n",
    "    \"create_env_on_driver\": True,\n",
    "    \"horizon\" : 252,\n",
    "    # Set Torch as Framework\n",
    "    \"framework\": \"torch\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2a13d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping criteria for tune\n",
    "stop = {\n",
    "    \"training_iteration\": 10,\n",
    "    \"episode_reward_mean\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "153ec068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "\n",
    "tf1, tf, tf_version = try_import_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b70f821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "\n",
    "class MyTorchModel(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        \"\"\"Build a simple [16, 16]-MLP (+ value branch).\"\"\"\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs,\n",
    "                              model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.device = torch.device(\"cuda\"\n",
    "                                   if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Hidden layer (shared by action logits outputs and value output).\n",
    "        self.layer_1 = nn.Linear(obs_space.shape[0], 16).to(self.device)\n",
    "\n",
    "        # Action logits output.\n",
    "        self.layer_out = nn.Linear(16, num_outputs).to(self.device)\n",
    "\n",
    "        # \"Value\"-branch (single node output).\n",
    "        # Used by several RLlib algorithms (e.g. PPO) to calculate an observation's value.\n",
    "        self.value_branch = nn.Linear(16, 1).to(self.device)\n",
    "        self.cur_value = None\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \"\"\"Custom-define your forard pass logic here.\"\"\"\n",
    "        # Pass inputs through our 2 layers.\n",
    "        layer_1_out = self.layer_1(input_dict[\"obs\"])\n",
    "        logits = self.layer_out(layer_1_out)\n",
    "\n",
    "        # Calculate the \"value\" of the observation and store it for\n",
    "        # when `value_function` is called.\n",
    "        self.cur_value = self.value_branch(layer_1_out).squeeze(1)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        \"\"\"Implement the value branch forward pass logic here:\n",
    "        \n",
    "        We will just return the already calculated `self.cur_value`.\n",
    "        \"\"\"\n",
    "        assert self.cur_value is not None, \"Must call `forward()` first!\"\n",
    "        return self.cur_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "831b9b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our custom model and re-run the experiment.\n",
    "config.update({\n",
    "    \"model\": {\n",
    "        \"custom_model\": MyTorchModel,  # for torch users: \"custom_model\": MyTorchModel\n",
    "        \"custom_model_config\": {\n",
    "            #\"layers\": [128, 128],\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57542302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'env': ray_trading_env.RayTradingEnvironment,\n",
       " 'create_env_on_driver': True,\n",
       " 'horizon': 252,\n",
       " 'framework': 'torch',\n",
       " 'model': {'custom_model': __main__.MyTorchModel, 'custom_model_config': {}}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f57f84d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=58451)\u001b[0m INFO:ray_trading_env:ray_trading_env logger started.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=58451)\u001b[0m 2022-06-27 19:12:51,955\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=58451)\u001b[0m 2022-06-27 19:12:51,956\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=58451)\u001b[0m INFO:ray_trading_env:got data for AAPL...\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=58451)\u001b[0m /opt/conda/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=58451)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=58451)\u001b[0m 2022-06-27 19:12:52,126\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-06-27 19:12:52 (running for 00:00:03.31)<br>Memory usage on this node: 49.1/251.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/24 CPUs, 0/2 GPUs, 0.0/170.55 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:A100)<br>Result logdir: /home/jovyan/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                           </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_RayTradingEnvironment_22415_00000</td><td>RUNNING </td><td>10.1.150.226:58451</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=58451)\u001b[0m 2022-06-27 19:12:52,237\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=58486)\u001b[0m INFO:ray_trading_env:ray_trading_env logger started.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=58485)\u001b[0m INFO:ray_trading_env:ray_trading_env logger started.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=58486)\u001b[0m INFO:ray_trading_env:got data for AAPL...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=58485)\u001b[0m INFO:ray_trading_env:got data for AAPL...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=58486)\u001b[0m /opt/conda/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=58486)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=58486)\u001b[0m 2022-06-27 19:12:54,637\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=58485)\u001b[0m /opt/conda/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=58485)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=58485)\u001b[0m 2022-06-27 19:12:54,638\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-06-27 19:12:57 (running for 00:00:08.31)<br>Memory usage on this node: 49.5/251.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/24 CPUs, 0/2 GPUs, 0.0/170.55 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:A100)<br>Result logdir: /home/jovyan/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                           </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_RayTradingEnvironment_22415_00000</td><td>RUNNING </td><td>10.1.150.226:58451</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_RayTradingEnvironment_22415_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-06-27_19-12-57\n",
      "  done: false\n",
      "  episode_len_mean: 252.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.19450103561436083\n",
      "  episode_reward_mean: -0.4151397341002601\n",
      "  episode_reward_min: -0.9863045778310642\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 14\n",
      "  experiment_id: caa23fb1a9534bc38ba90c04332b197e\n",
      "  hostname: jupyter-fewald\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.034536043674715\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00040081199754314314\n",
      "          policy_loss: -0.005791019055471625\n",
      "          total_loss: 0.06494033922972058\n",
      "          vf_explained_var: -0.9039767412088251\n",
      "          vf_loss: 0.07065119634492584\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.1.150.226\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.6375\n",
      "    ram_util_percent: 19.65\n",
      "  pid: 58451\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03369970955531757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1490590930997819\n",
      "    mean_inference_ms: 0.3936831680671506\n",
      "    mean_raw_obs_processing_ms: 0.06287893851002356\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 252.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.19450103561436083\n",
      "    episode_reward_mean: -0.4151397341002601\n",
      "    episode_reward_min: -0.9863045778310642\n",
      "    episodes_this_iter: 14\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      episode_reward:\n",
      "      - -0.9863045778310642\n",
      "      - -0.3408197038129197\n",
      "      - -0.295893014099761\n",
      "      - -0.9353226801175604\n",
      "      - -0.3872320926453374\n",
      "      - -0.3772287022961006\n",
      "      - -0.2347447629081802\n",
      "      - -0.48607597741500963\n",
      "      - -0.3737731266073868\n",
      "      - 0.19450103561436083\n",
      "      - -0.28868280787169603\n",
      "      - -0.32737087058603587\n",
      "      - -0.9246861957460751\n",
      "      - -0.04832280108087689\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.03369970955531757\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1490590930997819\n",
      "      mean_inference_ms: 0.3936831680671506\n",
      "      mean_raw_obs_processing_ms: 0.06287893851002356\n",
      "  time_since_restore: 5.323890686035156\n",
      "  time_this_iter_s: 5.323890686035156\n",
      "  time_total_s: 5.323890686035156\n",
      "  timers:\n",
      "    learn_throughput: 2653.933\n",
      "    learn_time_ms: 1507.197\n",
      "    load_throughput: 19065018.182\n",
      "    load_time_ms: 0.21\n",
      "    training_iteration_time_ms: 5319.932\n",
      "    update_time_ms: 1.182\n",
      "  timestamp: 1656357177\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: '22415_00000'\n",
      "  warmup_time: 0.28847312927246094\n",
      "  \n",
      "Result for PPO_RayTradingEnvironment_22415_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-06-27_19-13-03\n",
      "  done: false\n",
      "  episode_len_mean: 252.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4938807844966161\n",
      "  episode_reward_mean: -0.2806617890583772\n",
      "  episode_reward_min: -1.1700809068137037\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 46\n",
      "  experiment_id: caa23fb1a9534bc38ba90c04332b197e\n",
      "  hostname: jupyter-fewald\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.05000000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 1.013535228916394\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0005053075629157738\n",
      "          policy_loss: 0.0025967812586215233\n",
      "          total_loss: 0.032411545515060425\n",
      "          vf_explained_var: -0.6474754216850445\n",
      "          vf_loss: 0.029789498318687723\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 10.1.150.226\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.974999999999994\n",
      "    ram_util_percent: 19.7\n",
      "  pid: 58451\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.032874065973816216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14544071836696262\n",
      "    mean_inference_ms: 0.38452471635203833\n",
      "    mean_raw_obs_processing_ms: 0.0617959538022868\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 252.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.4938807844966161\n",
      "    episode_reward_mean: -0.2806617890583772\n",
      "    episode_reward_min: -1.1700809068137037\n",
      "    episodes_this_iter: 16\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      episode_reward:\n",
      "      - -0.9863045778310642\n",
      "      - -0.3408197038129197\n",
      "      - -0.295893014099761\n",
      "      - -0.9353226801175604\n",
      "      - -0.3872320926453374\n",
      "      - -0.3772287022961006\n",
      "      - -0.2347447629081802\n",
      "      - -0.48607597741500963\n",
      "      - -0.3737731266073868\n",
      "      - 0.19450103561436083\n",
      "      - -0.28868280787169603\n",
      "      - -0.32737087058603587\n",
      "      - -0.9246861957460751\n",
      "      - -0.04832280108087689\n",
      "      - -0.07265467296436375\n",
      "      - 0.01950559371623792\n",
      "      - -0.044888199276033214\n",
      "      - -0.26405840453060053\n",
      "      - -0.4306823108964005\n",
      "      - -0.11927491749687577\n",
      "      - -0.14833232507397076\n",
      "      - -0.2902807950001187\n",
      "      - -0.8716462276541913\n",
      "      - -0.3244479956166046\n",
      "      - -0.45608138507150275\n",
      "      - 0.4938807844966161\n",
      "      - -0.08554646500976502\n",
      "      - -0.2295708179014245\n",
      "      - 0.3496594612312484\n",
      "      - 0.3414896355504395\n",
      "      - 0.0828281029375838\n",
      "      - -0.3731962824479342\n",
      "      - 0.020053321865495424\n",
      "      - -0.37750258034269046\n",
      "      - 0.257477535971977\n",
      "      - -0.3999449192496975\n",
      "      - -0.626879512667327\n",
      "      - -0.36167881419415204\n",
      "      - -1.1700809068137037\n",
      "      - -0.509960924366876\n",
      "      - -0.3678459031939761\n",
      "      - 0.01180955301242485\n",
      "      - -0.283399911462445\n",
      "      - -0.5091039922632865\n",
      "      - -0.40625564384614843\n",
      "      - 0.048123899276356595\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.032874065973816216\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.14544071836696262\n",
      "      mean_inference_ms: 0.38452471635203833\n",
      "      mean_raw_obs_processing_ms: 0.0617959538022868\n",
      "  time_since_restore: 10.916860818862915\n",
      "  time_this_iter_s: 2.809492588043213\n",
      "  time_total_s: 10.916860818862915\n",
      "  timers:\n",
      "    learn_throughput: 2631.033\n",
      "    learn_time_ms: 1520.315\n",
      "    load_throughput: 4953902.362\n",
      "    load_time_ms: 0.807\n",
      "    training_iteration_time_ms: 3635.316\n",
      "    update_time_ms: 1.151\n",
      "  timestamp: 1656357183\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: '22415_00000'\n",
      "  warmup_time: 0.28847312927246094\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-06-27 19:13:03 (running for 00:00:14.33)<br>Memory usage on this node: 49.3/251.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/24 CPUs, 0/2 GPUs, 0.0/170.55 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:A100)<br>Result logdir: /home/jovyan/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                           </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_RayTradingEnvironment_22415_00000</td><td>RUNNING </td><td>10.1.150.226:58451</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         10.9169</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-0.280662</td><td style=\"text-align: right;\">            0.493881</td><td style=\"text-align: right;\">            -1.17008</td><td style=\"text-align: right;\">               252</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_RayTradingEnvironment_22415_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-06-27_19-13-09\n",
      "  done: true\n",
      "  episode_len_mean: 252.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.4938807844966161\n",
      "  episode_reward_mean: -0.29657268018874905\n",
      "  episode_reward_min: -1.1700809068137037\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 78\n",
      "  experiment_id: caa23fb1a9534bc38ba90c04332b197e\n",
      "  hostname: jupyter-fewald\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.012500000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.9753258644893605\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00032299408262023336\n",
      "          policy_loss: -0.004573205544022463\n",
      "          total_loss: 0.016017934673976515\n",
      "          vf_explained_var: -0.14484159754168602\n",
      "          vf_loss: 0.02058710190041932\n",
      "        model: {}\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_env_steps_sampled: 20000\n",
      "    num_env_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 10.1.150.226\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 20000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.56\n",
      "    ram_util_percent: 19.740000000000002\n",
      "  pid: 58451\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.032671093953802015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14470476742030478\n",
      "    mean_inference_ms: 0.3825543865271456\n",
      "    mean_raw_obs_processing_ms: 0.06173169773032017\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 252.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 0.4938807844966161\n",
      "    episode_reward_mean: -0.29657268018874905\n",
      "    episode_reward_min: -1.1700809068137037\n",
      "    episodes_this_iter: 16\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      - 252\n",
      "      episode_reward:\n",
      "      - -0.9863045778310642\n",
      "      - -0.3408197038129197\n",
      "      - -0.295893014099761\n",
      "      - -0.9353226801175604\n",
      "      - -0.3872320926453374\n",
      "      - -0.3772287022961006\n",
      "      - -0.2347447629081802\n",
      "      - -0.48607597741500963\n",
      "      - -0.3737731266073868\n",
      "      - 0.19450103561436083\n",
      "      - -0.28868280787169603\n",
      "      - -0.32737087058603587\n",
      "      - -0.9246861957460751\n",
      "      - -0.04832280108087689\n",
      "      - -0.07265467296436375\n",
      "      - 0.01950559371623792\n",
      "      - -0.044888199276033214\n",
      "      - -0.26405840453060053\n",
      "      - -0.4306823108964005\n",
      "      - -0.11927491749687577\n",
      "      - -0.14833232507397076\n",
      "      - -0.2902807950001187\n",
      "      - -0.8716462276541913\n",
      "      - -0.3244479956166046\n",
      "      - -0.45608138507150275\n",
      "      - 0.4938807844966161\n",
      "      - -0.08554646500976502\n",
      "      - -0.2295708179014245\n",
      "      - 0.3496594612312484\n",
      "      - 0.3414896355504395\n",
      "      - 0.0828281029375838\n",
      "      - -0.3731962824479342\n",
      "      - 0.020053321865495424\n",
      "      - -0.37750258034269046\n",
      "      - 0.257477535971977\n",
      "      - -0.3999449192496975\n",
      "      - -0.626879512667327\n",
      "      - -0.36167881419415204\n",
      "      - -1.1700809068137037\n",
      "      - -0.509960924366876\n",
      "      - -0.3678459031939761\n",
      "      - 0.01180955301242485\n",
      "      - -0.283399911462445\n",
      "      - -0.5091039922632865\n",
      "      - -0.40625564384614843\n",
      "      - 0.048123899276356595\n",
      "      - -0.7278984514164617\n",
      "      - -0.1459613787735783\n",
      "      - -0.8658215173561945\n",
      "      - 0.20191662671130092\n",
      "      - -0.1891098231910885\n",
      "      - -0.6355027871836132\n",
      "      - -0.28156067436323545\n",
      "      - -0.0020239056241265372\n",
      "      - -0.20918959617000432\n",
      "      - 0.00903886617658451\n",
      "      - -0.6373389363293479\n",
      "      - -0.27698166051754713\n",
      "      - 0.02901295469308677\n",
      "      - -0.6911920846174082\n",
      "      - -1.0668062415667827\n",
      "      - -0.7126590861310619\n",
      "      - -0.49744995514716417\n",
      "      - -0.2821504775265005\n",
      "      - -0.6883509711019784\n",
      "      - -0.19645369859899883\n",
      "      - -0.39344670440106927\n",
      "      - -0.5489069320704568\n",
      "      - -0.18941829572464247\n",
      "      - 0.12905880088670146\n",
      "      - -0.2763305245254773\n",
      "      - -0.029452629067824462\n",
      "      - -0.24470351497273624\n",
      "      - -0.45243752933673465\n",
      "      - -0.06287465018902838\n",
      "      - -0.38001579618166126\n",
      "      - -0.24285892947250515\n",
      "      - 0.3356427450524765\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.032671093953802015\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.14470476742030478\n",
      "      mean_inference_ms: 0.3825543865271456\n",
      "      mean_raw_obs_processing_ms: 0.06173169773032017\n",
      "  time_since_restore: 16.769867658615112\n",
      "  time_this_iter_s: 2.924656391143799\n",
      "  time_total_s: 16.769867658615112\n",
      "  timers:\n",
      "    learn_throughput: 2627.016\n",
      "    learn_time_ms: 1522.64\n",
      "    load_throughput: 3929642.573\n",
      "    load_time_ms: 1.018\n",
      "    training_iteration_time_ms: 3350.389\n",
      "    update_time_ms: 1.205\n",
      "  timestamp: 1656357189\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: '22415_00000'\n",
      "  warmup_time: 0.28847312927246094\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-06-27 19:13:09 (running for 00:00:20.25)<br>Memory usage on this node: 49.5/251.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/24 CPUs, 0/2 GPUs, 0.0/170.55 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:A100)<br>Result logdir: /home/jovyan/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                           </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_RayTradingEnvironment_22415_00000</td><td>TERMINATED</td><td>10.1.150.226:58451</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         16.7699</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-0.296573</td><td style=\"text-align: right;\">            0.493881</td><td style=\"text-align: right;\">            -1.17008</td><td style=\"text-align: right;\">               252</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-06-27 19:13:09 (running for 00:00:20.27)<br>Memory usage on this node: 49.5/251.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/24 CPUs, 0/2 GPUs, 0.0/170.55 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:A100)<br>Result logdir: /home/jovyan/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                           </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_RayTradingEnvironment_22415_00000</td><td>TERMINATED</td><td>10.1.150.226:58451</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         16.7699</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-0.296573</td><td style=\"text-align: right;\">            0.493881</td><td style=\"text-align: right;\">            -1.17008</td><td style=\"text-align: right;\">               252</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 19:13:09,866\tINFO tune.py:747 -- Total run time: 21.02 seconds (20.25 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f2330034760>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune.run(\n",
    "    \"PPO\",\n",
    "    config=config,  # for torch users: config=dict(config, **{\"framework\": \"torch\"}),\n",
    "    stop={\n",
    "        \"training_iteration\": 5,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9640fbae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
