{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a5e6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ray_trading_env:ray_trading_env logger started.\n"
     ]
    }
   ],
   "source": [
    "from ray_trading_env import RayTradingEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1947801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f47ccaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 14:25:04,985\tWARNING services.py:2002 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.8.8', ray_version='1.13.0', ray_commit='e4ce38d001dbbe09cd21c497fedd03d692b2be3e', address_info={'node_ip_address': '10.1.150.226', 'raylet_ip_address': '10.1.150.226', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-06-27_14-25-03_365442_27490/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-06-27_14-25-03_365442_27490/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2022-06-27_14-25-03_365442_27490', 'metrics_export_port': 60507, 'gcs_address': '10.1.150.226:58938', 'address': '10.1.150.226:58938', 'node_id': '8c27d2d27bfe09f6c3c9ab027b13f77e6e3d8d39f55f678c75ba0df9'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start a new instance of Ray\n",
    "# Note: execute cell only once!\n",
    "# instance can be stopped with ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "887e9204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config dict for Rllib\n",
    "config = {\n",
    "    \"env\": RayTradingEnvironment,\n",
    "    \"env_config\": {\n",
    "        \"config\": {\n",
    "            \"trading_days\": 252,\n",
    "            \"trading_cost_bps\": 1e-3,\n",
    "            \"time_cost_bps\": 1e-4,\n",
    "            \"ticker\": \"AAPL\",\n",
    "            \"get_data_from\": \"csv\", # \"csv\" or \"hdf\"\n",
    "            #\"max_episode_steps\": 252,\n",
    "        },\n",
    "    },\n",
    "\n",
    "    \"create_env_on_driver\": True,\n",
    "    # horizon needs to be specified if the env has no \n",
    "    # max_number_of_steps-like parameter\n",
    "    \"horizon\" : 252,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e41ad6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 14:25:06,222\tINFO trainer.py:2332 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2022-06-27 14:25:06,226\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-06-27 14:25:06,226\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "INFO:ray_trading_env:got data for AAPL...\n",
      "INFO:ray_trading_env:None\n",
      "/opt/conda/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n",
      "2022-06-27 14:25:06,307\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 9367 entries, (Timestamp('1981-01-30 00:00:00'), 'AAPL') to (Timestamp('2018-03-27 00:00:00'), 'AAPL')\n",
      "Data columns (total 10 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   returns  9367 non-null   float64\n",
      " 1   ret_2    9367 non-null   float64\n",
      " 2   ret_5    9367 non-null   float64\n",
      " 3   ret_10   9367 non-null   float64\n",
      " 4   ret_21   9367 non-null   float64\n",
      " 5   rsi      9367 non-null   float64\n",
      " 6   macd     9367 non-null   float64\n",
      " 7   atr      9367 non-null   float64\n",
      " 8   stoch    9367 non-null   float64\n",
      " 9   ultosc   9367 non-null   float64\n",
      "dtypes: float64(10)\n",
      "memory usage: 1.1+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 14:25:08,128\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Trainer object using above config.\n",
    "rllib_trainer = PPOTrainer(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "225459d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_workers': 2,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'create_env_on_driver': True,\n",
       " 'rollout_fragment_length': 200,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'gamma': 0.99,\n",
       " 'lr': 5e-05,\n",
       " 'train_batch_size': 4000,\n",
       " 'model': {'_use_default_native_models': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': False,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'use_attention': False,\n",
       "  'attention_num_transformer_units': 1,\n",
       "  'attention_dim': 64,\n",
       "  'attention_num_heads': 1,\n",
       "  'attention_head_dim': 32,\n",
       "  'attention_memory_inference': 50,\n",
       "  'attention_memory_training': 50,\n",
       "  'attention_position_wise_mlp_dim': 32,\n",
       "  'attention_init_gru_gate_bias': 2.0,\n",
       "  'attention_use_n_prev_actions': 0,\n",
       "  'attention_use_n_prev_rewards': 0,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1},\n",
       " 'optimizer': {},\n",
       " 'horizon': 252,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env': 'RayTradingEnvironment',\n",
       " 'observation_space': None,\n",
       " 'action_space': None,\n",
       " 'env_config': {'config': {'trading_days': 252,\n",
       "   'trading_cost_bps': 0.001,\n",
       "   'time_cost_bps': 0.0001,\n",
       "   'ticker': 'AAPL',\n",
       "   'get_data_from': 'csv'}},\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'record_env': False,\n",
       " 'clip_rewards': None,\n",
       " 'normalize_actions': True,\n",
       " 'clip_actions': False,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       " 'ignore_worker_failures': False,\n",
       " 'recreate_failed_workers': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'tf',\n",
       " 'eager_tracing': False,\n",
       " 'eager_max_retraces': 20,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'evaluation_interval': 0,\n",
       " 'evaluation_duration': 10,\n",
       " 'evaluation_duration_unit': 'episodes',\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {'num_workers': 2,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'create_env_on_driver': True,\n",
       "  'rollout_fragment_length': 200,\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'gamma': 0.99,\n",
       "  'lr': 5e-05,\n",
       "  'train_batch_size': 4000,\n",
       "  'model': {'_use_default_native_models': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': False,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'lstm_use_prev_action_reward': -1},\n",
       "  'optimizer': {},\n",
       "  'horizon': 252,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'env': 'RayTradingEnvironment',\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'env_config': {'config': {'trading_days': 252,\n",
       "    'trading_cost_bps': 0.001,\n",
       "    'time_cost_bps': 0.0001,\n",
       "    'ticker': 'AAPL',\n",
       "    'get_data_from': 'csv'}},\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'record_env': False,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'log_level': 'WARN',\n",
       "  'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "  'ignore_worker_failures': False,\n",
       "  'recreate_failed_workers': False,\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'framework': 'tf',\n",
       "  'eager_tracing': False,\n",
       "  'eager_max_retraces': 20,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'evaluation_interval': 0,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'in_evaluation': False,\n",
       "  'evaluation_config': {},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'custom_eval_function': None,\n",
       "  'always_attach_evaluation_results': False,\n",
       "  'keep_per_episode_custom_metrics': False,\n",
       "  'sample_async': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'compress_observations': False,\n",
       "  'metrics_episode_collection_timeout_s': 180,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_reporting': None,\n",
       "  'min_train_timesteps_per_reporting': None,\n",
       "  'min_sample_timesteps_per_reporting': 0,\n",
       "  'seed': None,\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_gpus': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'placement_strategy': 'PACK',\n",
       "  'input': 'sampler',\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_config': {},\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box(returns    -0.518692\n",
       "    ret_2     -13.186786\n",
       "    ret_5      -9.157841\n",
       "    ret_10     -6.979122\n",
       "    ret_21     -5.289787\n",
       "    rsi        -1.529044\n",
       "    macd       -5.407722\n",
       "    atr        -0.615589\n",
       "    stoch      -2.762308\n",
       "    ultosc     -3.964109\n",
       "    dtype: float32, returns     0.332152\n",
       "    ret_2      11.431712\n",
       "    ret_5      10.235379\n",
       "    ret_10      9.135829\n",
       "    ret_21      8.238228\n",
       "    rsi         1.499695\n",
       "    macd        5.705033\n",
       "    atr         5.415272\n",
       "    stoch       2.712635\n",
       "    ultosc      2.763141\n",
       "    dtype: float32, (10,), float32), action_space=Discrete(3), config={})},\n",
       "   'policy_map_capacity': 100,\n",
       "   'policy_map_cache': None,\n",
       "   'policy_mapping_fn': None,\n",
       "   'policies_to_train': None,\n",
       "   'observation_fn': None,\n",
       "   'replay_mode': 'independent',\n",
       "   'count_steps_by': 'env_steps'},\n",
       "  'logger_config': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_execution_plan_api': True,\n",
       "  'disable_env_checking': False,\n",
       "  'simple_optimizer': False,\n",
       "  'monitor': -1,\n",
       "  'evaluation_num_episodes': -1,\n",
       "  'metrics_smoothing_episodes': -1,\n",
       "  'timesteps_per_iteration': 0,\n",
       "  'min_iter_time_s': -1,\n",
       "  'collect_metrics_timeout': -1,\n",
       "  'lr_schedule': None,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'kl_coeff': 0.2,\n",
       "  'sgd_minibatch_size': 128,\n",
       "  'num_sgd_iter': 30,\n",
       "  'shuffle_sequences': True,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'grad_clip': None,\n",
       "  'kl_target': 0.01,\n",
       "  'lambda': 1.0},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'always_attach_evaluation_results': False,\n",
       " 'keep_per_episode_custom_metrics': False,\n",
       " 'sample_async': False,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'metrics_episode_collection_timeout_s': 180,\n",
       " 'metrics_num_episodes_for_smoothing': 100,\n",
       " 'min_time_s_per_reporting': None,\n",
       " 'min_train_timesteps_per_reporting': None,\n",
       " 'min_sample_timesteps_per_reporting': 0,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 0,\n",
       " '_fake_gpus': False,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'placement_strategy': 'PACK',\n",
       " 'input': 'sampler',\n",
       " 'input_config': {},\n",
       " 'actions_in_input_normalized': False,\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_config': {},\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box(returns    -0.518692\n",
       "   ret_2     -13.186786\n",
       "   ret_5      -9.157841\n",
       "   ret_10     -6.979122\n",
       "   ret_21     -5.289787\n",
       "   rsi        -1.529044\n",
       "   macd       -5.407722\n",
       "   atr        -0.615589\n",
       "   stoch      -2.762308\n",
       "   ultosc     -3.964109\n",
       "   dtype: float32, returns     0.332152\n",
       "   ret_2      11.431712\n",
       "   ret_5      10.235379\n",
       "   ret_10      9.135829\n",
       "   ret_21      8.238228\n",
       "   rsi         1.499695\n",
       "   macd        5.705033\n",
       "   atr         5.415272\n",
       "   stoch       2.712635\n",
       "   ultosc      2.763141\n",
       "   dtype: float32, (10,), float32), action_space=Discrete(3), config={})},\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_map_cache': None,\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent',\n",
       "  'count_steps_by': 'env_steps'},\n",
       " 'logger_config': None,\n",
       " '_tf_policy_handles_more_than_one_loss': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " '_disable_execution_plan_api': True,\n",
       " 'disable_env_checking': False,\n",
       " 'simple_optimizer': False,\n",
       " 'monitor': -1,\n",
       " 'evaluation_num_episodes': -1,\n",
       " 'metrics_smoothing_episodes': -1,\n",
       " 'timesteps_per_iteration': 0,\n",
       " 'min_iter_time_s': -1,\n",
       " 'collect_metrics_timeout': -1,\n",
       " 'lr_schedule': None,\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'kl_coeff': 0.2,\n",
       " 'sgd_minibatch_size': 128,\n",
       " 'num_sgd_iter': 30,\n",
       " 'shuffle_sequences': True,\n",
       " 'vf_loss_coeff': 1.0,\n",
       " 'entropy_coeff': 0.0,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.3,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01,\n",
       " 'lambda': 1.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the configuration of the trainer instance\n",
    "rllib_trainer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a23b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m INFO:ray_trading_env:ray_trading_env logger started.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m INFO:ray_trading_env:ray_trading_env logger started.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m INFO:ray_trading_env:got data for AAPL...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m INFO:ray_trading_env:None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m /opt/conda/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m 2022-06-27 14:25:08,748\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m <class 'pandas.core.frame.DataFrame'>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m MultiIndex: 9367 entries, (Timestamp('1981-01-30 00:00:00'), 'AAPL') to (Timestamp('2018-03-27 00:00:00'), 'AAPL')\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m Data columns (total 10 columns):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m  #   Column   Non-Null Count  Dtype  \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m ---  ------   --------------  -----  \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m  0   returns  9367 non-null   float64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m  1   ret_2    9367 non-null   float64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m  2   ret_5    9367 non-null   float64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m  3   ret_10   9367 non-null   float64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m  4   ret_21   9367 non-null   float64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m  5   rsi      9367 non-null   float64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m  6   macd     9367 non-null   float64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m  7   atr      9367 non-null   float64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m  8   stoch    9367 non-null   float64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m  9   ultosc   9367 non-null   float64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m dtypes: float64(10)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28418)\u001b[0m memory usage: 1.1+ MB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m <class 'pandas.core.frame.DataFrame'>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m MultiIndex: 9367 entries, (Timestamp('1981-01-30 00:00:00'), 'AAPL') to (Timestamp('2018-03-27 00:00:00'), 'AAPL')\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m Data columns (total 10 columns):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m  #   Column   Non-Null Count  Dtype  \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m ---  ------   --------------  -----  \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m  0   returns  9367 non-null   float64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m  1   ret_2    9367 non-null   float64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m  2   ret_5    9367 non-null   float64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m  3   ret_10   9367 non-null   float64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m  4   ret_21   9367 non-null   float64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m  5   rsi      9367 non-null   float64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m  6   macd     9367 non-null   float64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m  7   atr      9367 non-null   float64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m  8   stoch    9367 non-null   float64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m  9   ultosc   9367 non-null   float64\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m dtypes: float64(10)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m memory usage: 1.1+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m INFO:ray_trading_env:got data for AAPL...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m INFO:ray_trading_env:None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m /opt/conda/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m   logger.warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28419)\u001b[0m 2022-06-27 14:25:08,795\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-06-27 14:25:10,690\tWARNING deprecation.py:46 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=1: R(\"return\")=-0.25545044336567463\n",
      "Iteration=2: R(\"return\")=-0.21998192067462713\n",
      "Iteration=3: R(\"return\")=-0.2488374324417533\n",
      "Iteration=4: R(\"return\")=-0.27422901396665567\n",
      "Iteration=5: R(\"return\")=-0.2565586904981205\n"
     ]
    }
   ],
   "source": [
    "# train the trainer instance\n",
    "for _ in range(10):\n",
    "    results = rllib_trainer.train()\n",
    "    print(f\"Iteration={rllib_trainer.iteration}: R(\\\"return\\\")={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c81492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aede650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
